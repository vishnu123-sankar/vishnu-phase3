# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WvJ_m6zXy0JFpUYeBSY104dsV-lC_nA9
"""

# Sentiment Analysis Pipeline for Social Media Data

import pandas as pd
import numpy as np
import re
import string
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('stopwords')
nltk.download('wordnet')

# Load data
file_path = "test.csv"
df = pd.read_csv(file_path, header=None, quoting=1, on_bad_lines='skip')
df.columns = ['text', 'author', 'metadata']

# Drop rows with null text
df = df.dropna(subset=['text'])

# Data Preprocessing
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # remove links
    text = re.sub(r'\@\w+|\#','', text)  # remove mentions and hashtags
    text = re.sub(r'[^A-Za-z\s]', '', text)  # remove punctuations
    text = re.sub(r'\s+', ' ', text).strip()  # remove extra whitespace
    return text

df['clean_text'] = df['text'].apply(clean_text)

# Tokenization and Lemmatization
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df['processed_text'] = df['clean_text'].apply(preprocess)

# For demonstration: generate synthetic sentiment labels (in real case, these should be provided)
df['sentiment'] = np.random.choice(['positive', 'neutral', 'negative'], size=len(df))

# EDA: Plot label distribution
sns.countplot(data=df, x='sentiment')
plt.title('Sentiment Distribution')
plt.show()

# Word cloud
text_combined = " ".join(df['processed_text'])
wordcloud = WordCloud(width=800, height=400).generate(text_combined)
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Word Cloud')
plt.show()

# Feature Engineering
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['processed_text'])
y = df['sentiment']

# Model Training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Evaluation
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(df)
df.head()

!pip install gradio==3.38.0

import gradio as gr

def predict_sentiment(text):
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions).item()

        # Assuming your labels are in a list called 'labels'
        labels = dataset['train'].features['label'].names
        predicted_label = labels[predicted_class]

        return predicted_label

iface = gr.Interface(
        fn=predict_sentiment,
        inputs=gr.inputs.Textbox(lines=2, placeholder="Enter text here..."),
        outputs="text",
        title="Sentiment Analysis",
        description="Enter text to analyze its sentiment."
    )

iface.launch()